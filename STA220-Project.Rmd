---
title: "STA 220 Project: Insert Title"
author: "Sandeep Nair, Alan Phan, Thommas Phan"
date: "March 20, 2024"
output:
  html_document:
    df_print: paged
    number_sections: yes
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

# Introduction / Background

In today's landscape of heightened concern for food safety and public health, regulatory bodies like the Sacramento County Environmental Management Department (SCEMD) play a crucial role in ensuring community well-being. Driven by a commitment to enhancing quality of life, SCEMD employs various strategies including education, surveillance, enforcement, and community service, with a focus on mitigating foodborne illnesses—a significant health and economic burden.

Millions suffer from foodborne illnesses annually in the US, prompting organizations like the Centers for Disease Control and Prevention (CDC) and the Food and Drug Administration (FDA) to identify key risk factors and interventions. These encompass issues such as improper temperature control, inadequate cooking, poor hygiene, and contaminated equipment, among others.

SCEMD conducts routine inspections of retail food establishments in Sacramento County, enforcing strict compliance with health and safety codes outlined in state and county regulations. These inspections are guided by comprehensive protocols, such as the Sacramento County Retail Food Code Inspection Guide, aimed at maintaining standardized practices across establishments.

Effective food safety regulation hinges on proactive measures to identify and mitigate risks within the food supply chain. SCEMD oversees a diverse range of establishments, implementing measures to uphold hygiene and sanitation standards while reducing the incidence of foodborne illnesses.

In addition to enforcement, SCEMD engages in community outreach and educational initiatives, fostering collaboration between stakeholders. These efforts aim to empower establishments and consumers with knowledge and resources for maintaining a safe food environment.

This project embarks on an exploratory data analysis, leveraging data scraped from SCEMD reports and Yelp. Through visualization and observation, the project aims to uncover insights into health code violations within Sacramento County’s food establishments over the past year, contributing to the broader discourse on food safety and regulatory enforcement.


# Data Scraping / Organization

All data was scraped via Python and processed as either lists or dictionaries, and made into a data frame for visualizations and report format writeup in R Markdown, in the following sections. Below will outline the courses of action taken to scrape and organize the data and any problems we faced during the process.

## Obtaining Links to Sacramento County Inspection Data for the Past 12 Months

To collect links to inspection data from the Sacramento County Environmental Management Department (SCEMD) for the previous 12 months, a systematic approach was devised, leveraging Selenium for web automation. Initially, the process involved selecting specific date ranges from the SCEMD's inspection database, which necessitated interaction with a calendar interface. Initially, the focus was on extracting data from the current and last month, facilitated by tabs on the calendar. However, this approach proved insufficient for capturing a comprehensive dataset.

Refinement of the date range selection process was crucial to obtaining a broader dataset, encompassing inspections from March 2023 to the present. This required a more intricate methodology. By identifying the XPath for the "Previous" and "Next" arrow buttons on the calendar interface, the navigation process was automated. By manually navigating through the calendar and observing the number of clicks required to reach specific dates, a systematic approach was devised. Subsequently, Selenium was utilized to programmatically perform the requisite number of clicks on the arrow buttons to navigate to the desired dates.

Handling dynamic page loading was another challenge encountered during the data collection process. Upon selecting specific dates, it was noted that the inspection data did not load entirely, necessitating interaction with a "Load More" button. To address this, a while loop was implemented to iteratively click the "Load More" button until all inspection data was loaded.

Once the complete inspection data was loaded, each inspection's URL was extracted for further processing. Utilizing CSS selectors, the "View" buttons corresponding to each inspection were targeted, as XPath did not consistently retrieve the necessary information. The attributes (links) associated with each "View" button were then saved into a list for subsequent analysis.

By meticulously executing the outlined steps, a comprehensive collection of links to Sacramento County inspection data for the past 12 months was obtained. This process laid the groundwork for subsequent data processing and analysis, enabling insights into health code violations within the county's food establishments.

## Scraping Data from the Obtained Links to Inspection Data

In this phase of the project, the focus shifted towards extracting relevant information from the links obtained in the previous step, which led to summaries of observations and corrective actions from specific health inspections conducted at various restaurants. Each page was scraped using BeautifulSoup, a Python library for parsing HTML and XML documents. The process involved retrieving details such as the establishment name, inspection date, address, and health code violations.

One significant consideration was the possibility of multiple health inspections conducted at the same restaurant over the course of a year. Therefore, the scraping process needed to account for this potential repetition of data.

The scraping process was executed by iterating through each URL obtained from the previous step and extracting the required information. The key steps involved in the scraping process were as follows:

1. **Establishment Information Extraction:**
   - The BeautifulSoup library was employed to parse the HTML content of each page.
   - The establishment name and address were extracted from the designated HTML elements.

2. **Inspection Date Retrieval:**
   - Inspection dates were located within specific HTML elements and extracted accordingly.

3. **Health Code Violations Extraction:**
   - Health code violations were identified within the inspection summaries.
   - Each violation was associated with a specific health code, which ranged from 1 to 49. Sometimes the number had sub-letters, such as 1b and 1c.
   - To streamline analysis, only the primary code for each violation was retained.
   - A dictionary mapping each health code to its corresponding description was manually inputted from a PDF provided by the Sacramento County titled "Retail Food Inspection Guide."

4. **Handling Large Volume of Links:**
   - Given the substantial volume of links (nearly 10,000), the scraping process was divided into manageable chunks (500-1000 links at a time) to mitigate the risk of failure.
   - To prevent potential server blocks or interruptions, a sleep timer of 1 second was implemented between requests.

5. **Error Handling and Data Cleanup:**
   - Throughout the scraping process, error handling mechanisms were in place to identify and address any issues encountered.
   - Approximately 15 links were identified as faulty, lacking the establishment name, and subsequently discarded from the dataset.

6. **Data Storage and Merging:**
   - Results from each scraping iteration were saved as JSON files for further processing.
   - At the conclusion of the scraping process, the collected data from all iterations were merged to form a comprehensive dataset for subsequent analysis.

The provided code snippet serves as a reference for the scraping process, showcasing the implementation of BeautifulSoup to extract relevant data from individual URLs. By systematically executing the scraping process and meticulously handling data intricacies, a robust dataset was assembled, laying the groundwork for subsequent exploratory data analysis.

## Obtaining Supplementary Yelp Data

In order to augment our dataset with additional information about food establishments in the greater Sacramento area, we utilized the Yelp Fusion API. This API allows for querying establishments based on various parameters such as location, cuisine type, and price level. Our objective was to gather supplementary data on restaurants, grocery stores, and convenience stores—any establishments selling food items.

Here's an overview of the process:

1. **API Querying:**
   - We employed the Yelp Fusion API to conduct search queries for restaurants, grocery stores, and convenience stores in the greater Sacramento area.
   - Each establishment's search query results provided information such as rating, review count, price level, cuisine type, address, and coordinates.

2. **Handling API Limits:**
   - The Yelp API imposes query limits, allowing only up to 1000 establishments per query. To circumvent this limitation and obtain a comprehensive list, we executed multiple queries with different parameters.
   
3. **Optimizing Query Results:**
   - The search queries return results based on location and a sorting algorithm. We utilized four types of sorting operations: distance, rating counts, ratings, and best matches.
   - To maximize the number of results, we conducted queries for multiple locations across the greater Sacramento region, including major cities like Elk Grove, Citrus Heights, and Folsom.

4. **Data Cleaning and Filtering:**
   - After aggregating results from all queries, we noticed that some establishments, particularly those offering only takeout and delivery services, did not have valid addresses listed. As these were not relevant to our analysis, we removed them from the dataset.

5. **Result Summary:**
   - In total, we gathered Yelp metadata for 4283 establishments, providing valuable supplementary information to enrich our dataset.

By obtaining supplementary Yelp data, we aimed to enhance our understanding of the food landscape in the greater Sacramento area, complementing the insights derived from SCEMD inspection reports. This additional information will facilitate a more comprehensive analysis of factors influencing food safety and consumer preferences within the region.

## Creating a Key from our Datasets to Merge

In order to merge our datasets effectively, we needed to devise a unique identifier for each restaurant. This presented a challenge as traditional identifiers such as name, address, and zip code were not suitable due to the presence of multiple establishments with similar or identical attributes. After careful consideration, we settled on a combination of the first four numbers from the establishment's address and the first three letters of its name.

This approach offered a high degree of uniqueness, as it was highly improbable for two establishments with the same name to share the exact same four-number address. By incorporating both elements, we aimed to create a robust key that would facilitate accurate merging of our datasets.

Upon merging the datasets, we encountered a common issue where establishments appeared in multiple rows due to having multiple observations (e.g., health inspection data) associated with them. This occurred particularly because the health inspection data was scraped using dates, leading to multiple inspection records for the same restaurant.

To address this, we implemented a group-by operation followed by an aggregation function. By grouping the data by our established key, we were able to aggregate multiple observations for each establishment. Specifically, we retained the first observation for duplicate data points such as name, latitude, longitude, rating, price, etc., while consolidating different observations into a list. For instance, health code violations and inspection dates were aggregated into lists for each establishment.

This process resulted in a streamlined dataset with a reduced number of observations, from 3.6k to 2.3k. By consolidating duplicate entries and organizing the data systematically, we ensured the integrity and accuracy of our merged dataset, laying the groundwork for subsequent analysis and insights into the food landscape in the greater Sacramento area.

# Data Visualization

```{r, echo=False}
install.packes('reticulate')
library(reticulate)
use_condaenv("base")
```


```{python}
import pandas as pd


df = pd.read_csv('Data/merged.csv')

```

```{python}
df
```





# Discussion